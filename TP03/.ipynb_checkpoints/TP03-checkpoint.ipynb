{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> TP 3: Logistic Regression<br> <small>RÃ©da DEHAK<br> 22 November 2018</small> </center>\n",
    "\n",
    "The goal of this lab is :\n",
    "    - Test the logistic regression on classification problems\n",
    "    \n",
    "We will use the [Wine dataset](https://archive.ics.uci.edu/ml/datasets/Wine) from UCI. These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of thirteen constituents found in each of the three types of wines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Plotting Data\n",
    " \n",
    "First, we will use only two features from the data set: alcohol and ash (We can plot the solution in 2D space). The labels are supplied as an array of data with values from 1 to 3, but at first, we want a simple binary regression problem with a yes or no answer.  \n",
    "\n",
    "We filter the data set, reducing it to only include wines with labels 1 or 2.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "data = pd.read_csv('data.txt')\n",
    "\n",
    "reduced = data[data['class'] <= 2]\n",
    "X = reduced.as_matrix(columns=['alcohol', 'ash'])\n",
    "y = label_binarize(reduced['class'].values, [1, 2])[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.25)\n",
    "print('train:', len(Xtrain), 'test:', len(Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "MARKERS = ['+', 'x', '.']\n",
    "COLORS = ['red', 'green', 'blue']\n",
    "\n",
    "def plot_points(xy, labels):\n",
    "    \n",
    "    for i, label in enumerate(set(labels)):\n",
    "        points = np.array([xy[j,:] for j in range(len(xy)) if labels[j] == label])\n",
    "        marker = MARKERS[i % len(MARKERS)]\n",
    "        color = COLORS[i % len(COLORS)]\n",
    "        plt.scatter(points[:,0], points[:,1], marker=marker, color=color)\n",
    "\n",
    "plot_points(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we can plot line that could divide the two colored points with a small amount of error.\n",
    "\n",
    "# Logistic Regression\n",
    "\n",
    "To implement logistic regression, we need to define the cost function $J(\\theta)$, and compute the partial derivatives of $J(\\theta)$. As we have seen previously:\n",
    "\n",
    "$$\n",
    "J(\\theta) =-\\frac{1}{N}\\sum_{i=1}^{N}y^{i}\\log(f_\\theta(x^{i}))+(1-y^{i})\\log(1-f_\\theta(x^{i}))\n",
    "$$\n",
    "\n",
    "where $f_theta(x)$ is the logistic function\n",
    "\n",
    "$$\n",
    "f_\\theta(x) = \\frac{1}{1 + e^{-\\theta^Tx}}\n",
    "$$\n",
    "\n",
    "- Compute the partiel derivatives of $J(\\theta)$ and define the update formula of the gradient descent algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{dJ(\\theta)}{d\\theta} = \\frac{1}{N}\\sum_{i=1}^N (f_\\theta(x^i) - y^i)x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write a function LogRegTrain(x, y, num_epochs, learning_rate = 0.01) which compute $\\theta$ that minimize $J(\\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(w, x):\n",
    "    return (1 / (1 + np.exp(-x.dot(w))))\n",
    "\n",
    "def cost(w, x, y):\n",
    "    wx = x.dot(w)\n",
    "    return -np.sum(np.multiply((y - 1), wx) - np.log(1 + np.exp(-wx))) / len(y) \n",
    "\n",
    "def LogRegTrain(x, y, num_epochs, learning_rate, initw = None):\n",
    "    N = x.shape[0]\n",
    "    x = np.c_[np.ones((N, 1)), x]\n",
    "    d = x.shape[1]\n",
    "    y = y.reshape((N, 1))\n",
    "    if (initw == None):\n",
    "        w = np.zeros((x.shape[1], 1))\n",
    "    else:\n",
    "        w = initw\n",
    "        \n",
    "    for i in range(num_epochs):\n",
    "        yhat = logistic(w, x)\n",
    "        j = cost(w, x, y)\n",
    "        error = yhat.reshape((N,1)) - y.reshape((N,1))\n",
    "        if ((i % 100) == 0):\n",
    "            print('Iteration : ', i, 'Error -LLK : ', j, ' Error MSE :', np.linalg.norm(error))\n",
    "        dj = x.T.dot(error)/N\n",
    "        w = w - learning_rate[i] * dj    \n",
    "    return w\n",
    "\n",
    "def LogRegTrainNewton(x, y, precision = .01, initw = None):\n",
    "    N = x.shape[0]\n",
    "    x = np.c_[np.ones((N, 1)), x]\n",
    "    d = x.shape[1]\n",
    "    y = y.reshape((N, 1))\n",
    "    if (initw == None):\n",
    "        w = np.zeros((x.shape[1], 1))\n",
    "    else:\n",
    "        w = initw\n",
    "        \n",
    "    diff = precision + 1\n",
    "    i = 0;\n",
    "    while (diff > precision):\n",
    "        yhat = logistic(w, x)\n",
    "        j = cost(w, x, y)\n",
    "        error = yhat.reshape((N,1)) - y.reshape((N,1))\n",
    "        print('Iteration : ', i, 'Error -LLK : ', j, ' Error MSE :', np.linalg.norm(error))\n",
    "        dj = x.T.dot(error)/N\n",
    "        d2j = x.T.dot(np.diag(np.multiply(yhat, (1-yhat)).reshape(N))).dot(x)\n",
    "        delta = np.linalg.inv(d2j) @ dj\n",
    "        w = w - delta\n",
    "        diff = np.linalg.norm(delta)\n",
    "        i = i + 1\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#W = LogRegTrain(Xtrain, ytrain, 100000, .01 * np.ones((100000, 1)))\n",
    "W = LogRegTrainNewton(Xtrain, ytrain)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plot the boundary and checks that it is linear? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "def predict(X, theta):\n",
    "    X = np.c_[np.ones((len(X), 1)), X]\n",
    "    return (logistic(W, X) >= 0.5).astype(int)\n",
    "\n",
    "def plot_boundary(X, pred):\n",
    "    \n",
    "    x_min, x_max = X[:,0].min() - .1, X[:,0].max() + .1\n",
    "    y_min, y_max = X[:,1].min() - .1, X[:,1].max() + .1\n",
    "    \n",
    "    xs, ys = np.meshgrid(\n",
    "        np.linspace(x_min, x_max, 200),\n",
    "        np.linspace(y_min, y_max, 200)\n",
    "    )\n",
    "\n",
    "    xys = np.column_stack([xs.ravel(), ys.ravel()])\n",
    "    zs = pred(xys).reshape(xs.shape)\n",
    "    plt.contour(xs, ys, zs, colors='black')\n",
    "  \n",
    "plot_boundary(Xtrain, lambda x: predict(x, W))\n",
    "plot_points(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "predictions = predict(Xtest, W)\n",
    "print('accuracy:', accuracy_score(ytest, predictions))\n",
    "print('precision:', precision_score(ytest, predictions, average='macro'))\n",
    "print('recall:', recall_score(ytest, predictions, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How can we obtain a quadratic boundary? check it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(x):\n",
    "    return np.c_[x[:, 0], x[:, 1], x[:, 0] ** 2, x[:, 1] ** 2, np.multiply(x[:, 0], x[:,1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#W = LogRegTrain(Xtrain, ytrain, 100000, .01)\n",
    "W = LogRegTrainNewton(transform(Xtrain), ytrain)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_points(Xtrain, ytrain)\n",
    "plot_boundary(Xtrain, lambda x: predict(transform(x), W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(transform(Xtest), W)\n",
    "print('accuracy:', accuracy_score(ytest, predictions))\n",
    "print('precision:', precision_score(ytest, predictions, average='macro'))\n",
    "print('recall:', recall_score(ytest, predictions, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Logistic Regression\n",
    "\n",
    "The next step is something more interesting: we use a similar set of two features from the data set (this time alcohol and flavanoids), but with all three labels instead of two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.as_matrix(columns=['alcohol', 'flavanoids'])\n",
    "y = data.as_matrix(columns=['class'])\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.25)\n",
    "ytrain = label_binarize(ytrain, [1, 2, 3])\n",
    "\n",
    "plot_points(Xtrain, ytrain.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plotted data points again suggest some obvious linear boundaries between the three classes.\n",
    "\n",
    "We can solve this problem as three one-vs-all problems, and re-use all the previous code. In this part, we will try another solution inspired from softmax function known as softmax regression (See C.Bishop, \"Pattern Recognition and Machine Learning\", 2006, Springer).\n",
    "\n",
    "$$\n",
    "SoftMax_\\Theta(x, k) = \\frac{e^{\\theta_k^Tx}}{\\sum\\limits_{c=1}^K e^{\\theta_c^Tx}}\n",
    "$$\n",
    "\n",
    "- Propose a solution using this function and test it with linear and quadratic separator? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "Next, we want to include all the features from the data set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('class', 1).as_matrix()\n",
    "y = data.as_matrix(columns=['class'])\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.25)\n",
    "ytrain = label_binarize(ytrain, [1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are now significantly increasing the number of features, we apply regularisation  as part of new cost and gradient functions.  As we have seen with linear regression, regularization prevents overfitting, a situation where a large number of features allows the classifier to fit the training set *too* exactly, meaning that it fails to generalize well and perform accurately on data it hasn't yet seen.\n",
    "\n",
    "To avoid this problem, we add an additional term to the cost function\n",
    "\n",
    "$$\n",
    "J(\\theta) =-\\frac{1}{N}\\sum_{i=1}^{N}[y^{i}\\log(f_\\theta(x^{i}))+(1-y^{i})\\log(1-f_\\theta(x^{i}))] + \\frac{\\lambda}{2}\\|\\theta\\|_2^2\n",
    "$$\n",
    "\n",
    "- Compute the partiel derivatives of $J(\\theta)$ and define the update formula of the gradient descent algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write a function that minimize $J(\\theta)$ and test it on the WINE dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compare with non regularized version?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Conclude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
